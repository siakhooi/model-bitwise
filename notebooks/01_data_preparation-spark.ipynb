{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1097b7",
   "metadata": {},
   "source": [
    "# Data Preparation for Model Training (Apache Spark)\n",
    "\n",
    "This notebook prepares the XOR logic gate dataset for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cea0b43",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col, count, when, isnan, isnull\n",
    "import os\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Preparation for Model Training\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5122fc",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf3bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = spark.read.csv('../data/raw/input.csv', header=True, inferSchema=True)\n",
    "print(f\"Dataset shape: ({df.count()}, {len(df.columns)})\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59eec03",
   "metadata": {},
   "source": [
    "## 3. Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc40a9",
   "metadata": {},
   "source": [
    "### Data Types (df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7900c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7894d7",
   "metadata": {},
   "source": [
    "### Data Info (df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577beab2",
   "metadata": {},
   "source": [
    "### Check for Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing_counts = df.select([\n",
    "    count(when(isnull(c) | isnan(c), c)).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "missing_counts.show()\n",
    "\n",
    "total_missing = sum([\n",
    "    df.filter(isnull(col(c)) | isnan(col(c))).count()\n",
    "    for c in df.columns\n",
    "])\n",
    "print(f\"\\nTotal missing values: {total_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8ac74",
   "metadata": {},
   "source": [
    "### Statistical summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76e408",
   "metadata": {},
   "source": [
    "## 4. Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15121612",
   "metadata": {},
   "source": [
    "### 4.1 Convert columns to DoubleType (Spark standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e64053",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    df = df.withColumn(column, col(column).cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78bc1f2",
   "metadata": {},
   "source": [
    "## 5. Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb690ae3",
   "metadata": {},
   "source": [
    "### data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f4edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb6c7a",
   "metadata": {},
   "source": [
    "### data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d23bb",
   "metadata": {},
   "source": [
    "### df (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9761ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eda2450",
   "metadata": {},
   "source": [
    "## 6. Save Prepared Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc8ed1c",
   "metadata": {},
   "source": [
    "### prepare output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ba666",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b218d",
   "metadata": {},
   "source": [
    "### Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(output_dir, 'prepared_data.csv_spark')\n",
    "# Spark writes to a directory, so we use coalesce(1) to get a single file\n",
    "df.coalesce(1).write.mode('overwrite').option('header', 'true').csv(csv_path)\n",
    "print(f\"✓ Saved CSV: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc28472",
   "metadata": {},
   "source": [
    "### Save as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4fcd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = os.path.join(output_dir, 'prepared_data.parquet_spark')\n",
    "df.coalesce(1).write.mode('overwrite').parquet(parquet_path)\n",
    "print(f\"✓ Saved Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ac11d",
   "metadata": {},
   "source": [
    "### Verify saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02526383",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saved files/directories:\")\n",
    "for f in os.listdir(output_dir):\n",
    "    filepath = os.path.join(output_dir, f)\n",
    "    if os.path.isdir(filepath):\n",
    "        # For Spark output directories, show contents\n",
    "        total_size = sum(os.path.getsize(os.path.join(filepath, sf)) for sf in os.listdir(filepath))\n",
    "        print(f\"  - {f}/ ({total_size} bytes)\")\n",
    "    else:\n",
    "        size = os.path.getsize(filepath)\n",
    "        print(f\"  - {f} ({size} bytes)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
